# TextSummarization
In his 1958 paper "The automatic creation of literature abstracts" H.P. Luhn hypothesized hypothesized that the more
often words appear, the more crucial they are to the text's meaning. This is what is done in the following code:
it extracts the frequencies of each word in the sentences and retains the sentences with the highest frequency scores.
This is a simple, working technique, but it is not state-of-the-art. Other advanced text summarizers would use
word embedding to extract semantic significance.
For example, see the paper "Text Document Summarization Using Word Embedding" by Mudassir Mohd et al.

I might try to make one text summarization program algorithm using word embedding in the future.
